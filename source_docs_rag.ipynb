{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Medusa E-Comm RAG Docs\n",
    "\n",
    "***\n",
    "### **Last Updated: January 12, 2024**\n",
    "\n",
    "#### todo notes:\n",
    "\n",
    "I think we're seeing a version 3.0 model where it has a local PostgreSQL data-frame and fast embedding's preventing rate-limits. To do this, we'd have to convert from using the csv files existing, and rewrite our embedding code. With the latest updates to both the core Langchain Library, and OpenAI major updates with version 1.0.0 there will be plenty of room for advancement. Our current generations are using OpenAI API version 0.28. \n",
    "\n",
    "1.   update OpenAI version & corresponding embedding functions \n",
    "2.   update Langchain errors which show depricated modules being used and new Langchain-Openai moduels \n",
    "3.   need to add streaming to the output text.\n",
    "4.   make corrections/ updates to langsmith (as needed)\n",
    "5.   convert existing pandas.df to local Postfresql server/ possibly log chat sequence and enhance conversational history \n",
    "\n",
    "#### questions/ answers history:\n",
    "i. as it stands, the memory is stored in the application's runtime memory. This means that the memory exists as long as the application is running and will be lost once the application is terminated. There is no standalone chat-history so it's best-practice to take notes as development continues. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting deeplake[enterprise]\n",
      "  Using cached deeplake-3.8.14-py3-none-any.whl\n",
      "\u001b[33mWARNING: deeplake 3.8.14 does not provide the extra 'enterprise'\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting numpy (from deeplake[enterprise])\n",
      "  Obtaining dependency information for numpy from https://files.pythonhosted.org/packages/94/9c/f1e88764737c126637d0434df712b1baa371a404a3e3751ee997e74e164b/numpy-1.26.3-cp312-cp312-macosx_11_0_arm64.whl.metadata\n",
      "  Using cached numpy-1.26.3-cp312-cp312-macosx_11_0_arm64.whl.metadata (61 kB)\n",
      "Collecting pillow (from deeplake[enterprise])\n",
      "  Obtaining dependency information for pillow from https://files.pythonhosted.org/packages/9d/a0/28756da34d6b58c3c5f6c1d5589e4e8f4e73472b55875524ae9d6e7e98fe/pillow-10.2.0-cp312-cp312-macosx_11_0_arm64.whl.metadata\n",
      "  Using cached pillow-10.2.0-cp312-cp312-macosx_11_0_arm64.whl.metadata (9.7 kB)\n",
      "Collecting boto3 (from deeplake[enterprise])\n",
      "  Obtaining dependency information for boto3 from https://files.pythonhosted.org/packages/e3/f7/93a4ba1cd2cc4ee95f871b0890e4ed60e52365110a074e7265279750a736/boto3-1.34.18-py3-none-any.whl.metadata\n",
      "  Downloading boto3-1.34.18-py3-none-any.whl.metadata (6.6 kB)\n",
      "Collecting click (from deeplake[enterprise])\n",
      "  Obtaining dependency information for click from https://files.pythonhosted.org/packages/00/2e/d53fa4befbf2cfa713304affc7ca780ce4fc1fd8710527771b58311a3229/click-8.1.7-py3-none-any.whl.metadata\n",
      "  Using cached click-8.1.7-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting pathos (from deeplake[enterprise])\n",
      "  Obtaining dependency information for pathos from https://files.pythonhosted.org/packages/d8/08/ac94fa6f9eefe32963b8a54e573dab0dbc0d3df24fd34924bd9ce7eab7c4/pathos-0.3.1-py3-none-any.whl.metadata\n",
      "  Using cached pathos-0.3.1-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting humbug>=0.3.1 (from deeplake[enterprise])\n",
      "  Obtaining dependency information for humbug>=0.3.1 from https://files.pythonhosted.org/packages/c8/cc/c8129d6e9a1f473b5e90cf1b8eac43191a22657b9b673e02815548662270/humbug-0.3.2-py3-none-any.whl.metadata\n",
      "  Using cached humbug-0.3.2-py3-none-any.whl.metadata (6.8 kB)\n",
      "Collecting tqdm (from deeplake[enterprise])\n",
      "  Obtaining dependency information for tqdm from https://files.pythonhosted.org/packages/00/e5/f12a80907d0884e6dff9c16d0c0114d81b8cd07dc3ae54c5e962cc83037e/tqdm-4.66.1-py3-none-any.whl.metadata\n",
      "  Using cached tqdm-4.66.1-py3-none-any.whl.metadata (57 kB)\n",
      "Collecting lz4 (from deeplake[enterprise])\n",
      "  Obtaining dependency information for lz4 from https://files.pythonhosted.org/packages/53/4d/8e04ef75feff8848ba3c624ce81c7732bdcea5f8f994758afa88cd3d7764/lz4-4.3.3-cp312-cp312-macosx_11_0_arm64.whl.metadata\n",
      "  Using cached lz4-4.3.3-cp312-cp312-macosx_11_0_arm64.whl.metadata (3.7 kB)\n",
      "Collecting pyjwt (from deeplake[enterprise])\n",
      "  Obtaining dependency information for pyjwt from https://files.pythonhosted.org/packages/2b/4f/e04a8067c7c96c364cef7ef73906504e2f40d690811c021e1a1901473a19/PyJWT-2.8.0-py3-none-any.whl.metadata\n",
      "  Using cached PyJWT-2.8.0-py3-none-any.whl.metadata (4.2 kB)\n",
      "Collecting pydantic (from deeplake[enterprise])\n",
      "  Obtaining dependency information for pydantic from https://files.pythonhosted.org/packages/dd/b7/9aea7ee6c01fe3f3c03b8ca3c7797c866df5fecece9d6cb27caa138db2e2/pydantic-2.5.3-py3-none-any.whl.metadata\n",
      "  Using cached pydantic-2.5.3-py3-none-any.whl.metadata (65 kB)\n",
      "Collecting aioboto3>=10.4.0 (from deeplake[enterprise])\n",
      "  Obtaining dependency information for aioboto3>=10.4.0 from https://files.pythonhosted.org/packages/75/50/e56cabec21ee746e7245d0a3d87f4e8c788b5249486ad1351dbfc01249ff/aioboto3-12.1.0-py3-none-any.whl.metadata\n",
      "  Using cached aioboto3-12.1.0-py3-none-any.whl.metadata (8.8 kB)\n",
      "Requirement already satisfied: nest-asyncio in ./env/lib/python3.12/site-packages (from deeplake[enterprise]) (1.5.8)\n",
      "Collecting aiobotocore[boto3]==2.8.0 (from aioboto3>=10.4.0->deeplake[enterprise])\n",
      "  Obtaining dependency information for aiobotocore[boto3]==2.8.0 from https://files.pythonhosted.org/packages/f9/5e/ca5fd1c417f6a1c8cde8519611d82faedb368d7e3684e3a6069c95721bdf/aiobotocore-2.8.0-py3-none-any.whl.metadata\n",
      "  Using cached aiobotocore-2.8.0-py3-none-any.whl.metadata (20 kB)\n",
      "Collecting botocore<1.33.2,>=1.32.4 (from aiobotocore[boto3]==2.8.0->aioboto3>=10.4.0->deeplake[enterprise])\n",
      "  Obtaining dependency information for botocore<1.33.2,>=1.32.4 from https://files.pythonhosted.org/packages/19/d2/9e0a21b8688f1ff1a27c7d806908fe324adba280c0f01c43cc80cc1a1c7e/botocore-1.33.1-py3-none-any.whl.metadata\n",
      "  Using cached botocore-1.33.1-py3-none-any.whl.metadata (6.1 kB)\n",
      "Collecting aiohttp<4.0.0,>=3.7.4.post0 (from aiobotocore[boto3]==2.8.0->aioboto3>=10.4.0->deeplake[enterprise])\n",
      "  Obtaining dependency information for aiohttp<4.0.0,>=3.7.4.post0 from https://files.pythonhosted.org/packages/cf/45/580b5a6abb70530cea7f6e697227c61e0001eff75d50b897a62b66c6d3b7/aiohttp-3.9.1-cp312-cp312-macosx_11_0_arm64.whl.metadata\n",
      "  Using cached aiohttp-3.9.1-cp312-cp312-macosx_11_0_arm64.whl.metadata (7.4 kB)\n",
      "Collecting wrapt<2.0.0,>=1.10.10 (from aiobotocore[boto3]==2.8.0->aioboto3>=10.4.0->deeplake[enterprise])\n",
      "  Obtaining dependency information for wrapt<2.0.0,>=1.10.10 from https://files.pythonhosted.org/packages/6a/d7/cfcd73e8f4858079ac59d9db1ec5a1349bc486ae8e9ba55698cc1f4a1dff/wrapt-1.16.0-cp312-cp312-macosx_11_0_arm64.whl.metadata\n",
      "  Using cached wrapt-1.16.0-cp312-cp312-macosx_11_0_arm64.whl.metadata (6.6 kB)\n",
      "Collecting aioitertools<1.0.0,>=0.5.1 (from aiobotocore[boto3]==2.8.0->aioboto3>=10.4.0->deeplake[enterprise])\n",
      "  Using cached aioitertools-0.11.0-py3-none-any.whl (23 kB)\n",
      "Collecting boto3 (from deeplake[enterprise])\n",
      "  Obtaining dependency information for boto3 from https://files.pythonhosted.org/packages/81/bf/1dccda444cede321eafc7ae880387df08fa958be080392032d5b72c6cddb/boto3-1.33.1-py3-none-any.whl.metadata\n",
      "  Using cached boto3-1.33.1-py3-none-any.whl.metadata (6.7 kB)\n",
      "Collecting jmespath<2.0.0,>=0.7.1 (from boto3->deeplake[enterprise])\n",
      "  Using cached jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
      "Collecting s3transfer<0.9.0,>=0.8.0 (from boto3->deeplake[enterprise])\n",
      "  Obtaining dependency information for s3transfer<0.9.0,>=0.8.0 from https://files.pythonhosted.org/packages/75/ca/5399536cbd5889ca4532d4b8bbcd17efa0fe0be0da04e143667a4ff5644e/s3transfer-0.8.2-py3-none-any.whl.metadata\n",
      "  Using cached s3transfer-0.8.2-py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting requests (from humbug>=0.3.1->deeplake[enterprise])\n",
      "  Obtaining dependency information for requests from https://files.pythonhosted.org/packages/70/8e/0e2d847013cb52cd35b38c009bb167a1a26b2ce6cd6965bf26b47bc0bf44/requests-2.31.0-py3-none-any.whl.metadata\n",
      "  Using cached requests-2.31.0-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting ppft>=1.7.6.7 (from pathos->deeplake[enterprise])\n",
      "  Obtaining dependency information for ppft>=1.7.6.7 from https://files.pythonhosted.org/packages/f0/f8/0a493dfdf73edbfe58cae1323aec72d0152f463c7a351bd285e9d500985c/ppft-1.7.6.7-py3-none-any.whl.metadata\n",
      "  Using cached ppft-1.7.6.7-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting dill>=0.3.7 (from pathos->deeplake[enterprise])\n",
      "  Obtaining dependency information for dill>=0.3.7 from https://files.pythonhosted.org/packages/f5/3a/74a29b11cf2cdfcd6ba89c0cecd70b37cd1ba7b77978ce611eb7a146a832/dill-0.3.7-py3-none-any.whl.metadata\n",
      "  Using cached dill-0.3.7-py3-none-any.whl.metadata (9.9 kB)\n",
      "Collecting pox>=0.3.3 (from pathos->deeplake[enterprise])\n",
      "  Obtaining dependency information for pox>=0.3.3 from https://files.pythonhosted.org/packages/17/c7/ef7e37e5a895f5de068b408a52bee0710b1092574b6b4ab247a767e9fbd5/pox-0.3.3-py3-none-any.whl.metadata\n",
      "  Using cached pox-0.3.3-py3-none-any.whl.metadata (8.0 kB)\n",
      "Collecting multiprocess>=0.70.15 (from pathos->deeplake[enterprise])\n",
      "  Obtaining dependency information for multiprocess>=0.70.15 from https://files.pythonhosted.org/packages/e7/41/96ac938770ba6e7d5ae1d8c9cafebac54b413549042c6260f0d0a6ec6622/multiprocess-0.70.15-py311-none-any.whl.metadata\n",
      "  Using cached multiprocess-0.70.15-py311-none-any.whl.metadata (7.2 kB)\n",
      "Collecting annotated-types>=0.4.0 (from pydantic->deeplake[enterprise])\n",
      "  Obtaining dependency information for annotated-types>=0.4.0 from https://files.pythonhosted.org/packages/28/78/d31230046e58c207284c6b2c4e8d96e6d3cb4e52354721b944d3e1ee4aa5/annotated_types-0.6.0-py3-none-any.whl.metadata\n",
      "  Using cached annotated_types-0.6.0-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting pydantic-core==2.14.6 (from pydantic->deeplake[enterprise])\n",
      "  Obtaining dependency information for pydantic-core==2.14.6 from https://files.pythonhosted.org/packages/a5/f8/07a2563f40b863ba97f3db648697f3f1d7b7edf1bd679f210064cb556e74/pydantic_core-2.14.6-cp312-cp312-macosx_11_0_arm64.whl.metadata\n",
      "  Using cached pydantic_core-2.14.6-cp312-cp312-macosx_11_0_arm64.whl.metadata (6.5 kB)\n",
      "Collecting typing-extensions>=4.6.1 (from pydantic->deeplake[enterprise])\n",
      "  Obtaining dependency information for typing-extensions>=4.6.1 from https://files.pythonhosted.org/packages/b7/f4/6a90020cd2d93349b442bfcb657d0dc91eee65491600b2cb1d388bc98e6b/typing_extensions-4.9.0-py3-none-any.whl.metadata\n",
      "  Using cached typing_extensions-4.9.0-py3-none-any.whl.metadata (3.0 kB)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in ./env/lib/python3.12/site-packages (from botocore<1.33.2,>=1.32.4->aiobotocore[boto3]==2.8.0->aioboto3>=10.4.0->deeplake[enterprise]) (2.8.2)\n",
      "Collecting urllib3<2.1,>=1.25.4 (from botocore<1.33.2,>=1.32.4->aiobotocore[boto3]==2.8.0->aioboto3>=10.4.0->deeplake[enterprise])\n",
      "  Obtaining dependency information for urllib3<2.1,>=1.25.4 from https://files.pythonhosted.org/packages/d2/b2/b157855192a68541a91ba7b2bbcb91f1b4faa51f8bae38d8005c034be524/urllib3-2.0.7-py3-none-any.whl.metadata\n",
      "  Using cached urllib3-2.0.7-py3-none-any.whl.metadata (6.6 kB)\n",
      "INFO: pip is looking at multiple versions of s3transfer to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting s3transfer<0.9.0,>=0.8.0 (from boto3->deeplake[enterprise])\n",
      "  Obtaining dependency information for s3transfer<0.9.0,>=0.8.0 from https://files.pythonhosted.org/packages/45/50/9a60e110bd85a7c082248bc707863f4f3c3ee3f37b89e6d03afac6a11354/s3transfer-0.8.0-py3-none-any.whl.metadata\n",
      "  Using cached s3transfer-0.8.0-py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting charset-normalizer<4,>=2 (from requests->humbug>=0.3.1->deeplake[enterprise])\n",
      "  Obtaining dependency information for charset-normalizer<4,>=2 from https://files.pythonhosted.org/packages/3a/52/9f9d17c3b54dc238de384c4cb5a2ef0e27985b42a0e5cc8e8a31d918d48d/charset_normalizer-3.3.2-cp312-cp312-macosx_11_0_arm64.whl.metadata\n",
      "  Using cached charset_normalizer-3.3.2-cp312-cp312-macosx_11_0_arm64.whl.metadata (33 kB)\n",
      "Collecting idna<4,>=2.5 (from requests->humbug>=0.3.1->deeplake[enterprise])\n",
      "  Obtaining dependency information for idna<4,>=2.5 from https://files.pythonhosted.org/packages/c2/e7/a82b05cf63a603df6e68d59ae6a68bf5064484a0718ea5033660af4b54a9/idna-3.6-py3-none-any.whl.metadata\n",
      "  Using cached idna-3.6-py3-none-any.whl.metadata (9.9 kB)\n",
      "Collecting certifi>=2017.4.17 (from requests->humbug>=0.3.1->deeplake[enterprise])\n",
      "  Obtaining dependency information for certifi>=2017.4.17 from https://files.pythonhosted.org/packages/64/62/428ef076be88fa93716b576e4a01f919d25968913e817077a386fcbe4f42/certifi-2023.11.17-py3-none-any.whl.metadata\n",
      "  Using cached certifi-2023.11.17-py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting attrs>=17.3.0 (from aiohttp<4.0.0,>=3.7.4.post0->aiobotocore[boto3]==2.8.0->aioboto3>=10.4.0->deeplake[enterprise])\n",
      "  Obtaining dependency information for attrs>=17.3.0 from https://files.pythonhosted.org/packages/e0/44/827b2a91a5816512fcaf3cc4ebc465ccd5d598c45cefa6703fcf4a79018f/attrs-23.2.0-py3-none-any.whl.metadata\n",
      "  Using cached attrs-23.2.0-py3-none-any.whl.metadata (9.5 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp<4.0.0,>=3.7.4.post0->aiobotocore[boto3]==2.8.0->aioboto3>=10.4.0->deeplake[enterprise])\n",
      "  Using cached multidict-6.0.4-cp312-cp312-macosx_10_9_universal2.whl\n",
      "Collecting yarl<2.0,>=1.0 (from aiohttp<4.0.0,>=3.7.4.post0->aiobotocore[boto3]==2.8.0->aioboto3>=10.4.0->deeplake[enterprise])\n",
      "  Obtaining dependency information for yarl<2.0,>=1.0 from https://files.pythonhosted.org/packages/54/99/ed3c92c38f421ba6e36caf6aa91c34118771d252dce800118fa2f44d7962/yarl-1.9.4-cp312-cp312-macosx_11_0_arm64.whl.metadata\n",
      "  Using cached yarl-1.9.4-cp312-cp312-macosx_11_0_arm64.whl.metadata (31 kB)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp<4.0.0,>=3.7.4.post0->aiobotocore[boto3]==2.8.0->aioboto3>=10.4.0->deeplake[enterprise])\n",
      "  Obtaining dependency information for frozenlist>=1.1.1 from https://files.pythonhosted.org/packages/3f/ab/c543c13824a615955f57e082c8a5ee122d2d5368e80084f2834e6f4feced/frozenlist-1.4.1-cp312-cp312-macosx_11_0_arm64.whl.metadata\n",
      "  Using cached frozenlist-1.4.1-cp312-cp312-macosx_11_0_arm64.whl.metadata (12 kB)\n",
      "Collecting aiosignal>=1.1.2 (from aiohttp<4.0.0,>=3.7.4.post0->aiobotocore[boto3]==2.8.0->aioboto3>=10.4.0->deeplake[enterprise])\n",
      "  Using cached aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
      "Requirement already satisfied: six>=1.5 in ./env/lib/python3.12/site-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.33.2,>=1.32.4->aiobotocore[boto3]==2.8.0->aioboto3>=10.4.0->deeplake[enterprise]) (1.16.0)\n",
      "Using cached aioboto3-12.1.0-py3-none-any.whl (32 kB)\n",
      "Using cached boto3-1.33.1-py3-none-any.whl (139 kB)\n",
      "Using cached humbug-0.3.2-py3-none-any.whl (15 kB)\n",
      "Using cached click-8.1.7-py3-none-any.whl (97 kB)\n",
      "Using cached lz4-4.3.3-cp312-cp312-macosx_11_0_arm64.whl (212 kB)\n",
      "Using cached numpy-1.26.3-cp312-cp312-macosx_11_0_arm64.whl (13.7 MB)\n",
      "Using cached pathos-0.3.1-py3-none-any.whl (82 kB)\n",
      "Using cached pillow-10.2.0-cp312-cp312-macosx_11_0_arm64.whl (3.3 MB)\n",
      "Using cached pydantic-2.5.3-py3-none-any.whl (381 kB)\n",
      "Using cached pydantic_core-2.14.6-cp312-cp312-macosx_11_0_arm64.whl (1.7 MB)\n",
      "Using cached PyJWT-2.8.0-py3-none-any.whl (22 kB)\n",
      "Using cached tqdm-4.66.1-py3-none-any.whl (78 kB)\n",
      "Using cached annotated_types-0.6.0-py3-none-any.whl (12 kB)\n",
      "Using cached botocore-1.33.1-py3-none-any.whl (11.6 MB)\n",
      "Using cached dill-0.3.7-py3-none-any.whl (115 kB)\n",
      "Using cached multiprocess-0.70.15-py311-none-any.whl (135 kB)\n",
      "Using cached pox-0.3.3-py3-none-any.whl (29 kB)\n",
      "Using cached ppft-1.7.6.7-py3-none-any.whl (56 kB)\n",
      "Using cached s3transfer-0.8.0-py3-none-any.whl (81 kB)\n",
      "Using cached typing_extensions-4.9.0-py3-none-any.whl (32 kB)\n",
      "Using cached requests-2.31.0-py3-none-any.whl (62 kB)\n",
      "Using cached aiohttp-3.9.1-cp312-cp312-macosx_11_0_arm64.whl (388 kB)\n",
      "Using cached certifi-2023.11.17-py3-none-any.whl (162 kB)\n",
      "Using cached charset_normalizer-3.3.2-cp312-cp312-macosx_11_0_arm64.whl (119 kB)\n",
      "Using cached idna-3.6-py3-none-any.whl (61 kB)\n",
      "Using cached urllib3-2.0.7-py3-none-any.whl (124 kB)\n",
      "Using cached wrapt-1.16.0-cp312-cp312-macosx_11_0_arm64.whl (38 kB)\n",
      "Using cached aiobotocore-2.8.0-py3-none-any.whl (75 kB)\n",
      "Using cached attrs-23.2.0-py3-none-any.whl (60 kB)\n",
      "Using cached frozenlist-1.4.1-cp312-cp312-macosx_11_0_arm64.whl (51 kB)\n",
      "Using cached yarl-1.9.4-cp312-cp312-macosx_11_0_arm64.whl (79 kB)\n",
      "Installing collected packages: wrapt, urllib3, typing-extensions, tqdm, pyjwt, ppft, pox, pillow, numpy, multidict, lz4, jmespath, idna, frozenlist, dill, click, charset-normalizer, certifi, attrs, annotated-types, aioitertools, yarl, requests, pydantic-core, multiprocess, botocore, aiosignal, s3transfer, pydantic, pathos, humbug, aiohttp, boto3, aiobotocore, aioboto3, deeplake\n",
      "Successfully installed aioboto3-12.1.0 aiobotocore-2.8.0 aiohttp-3.9.1 aioitertools-0.11.0 aiosignal-1.3.1 annotated-types-0.6.0 attrs-23.2.0 boto3-1.33.1 botocore-1.33.1 certifi-2023.11.17 charset-normalizer-3.3.2 click-8.1.7 deeplake-3.8.14 dill-0.3.7 frozenlist-1.4.1 humbug-0.3.2 idna-3.6 jmespath-1.0.1 lz4-4.3.3 multidict-6.0.4 multiprocess-0.70.15 numpy-1.26.3 pathos-0.3.1 pillow-10.2.0 pox-0.3.3 ppft-1.7.6.7 pydantic-2.5.3 pydantic-core-2.14.6 pyjwt-2.8.0 requests-2.31.0 s3transfer-0.8.0 tqdm-4.66.1 typing-extensions-4.9.0 urllib3-2.0.7 wrapt-1.16.0 yarl-1.9.4\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Collecting langchain\n",
      "  Obtaining dependency information for langchain from https://files.pythonhosted.org/packages/23/98/c70fac0f1b3193ced86013b563119c27c68ac26b684815f407555224108d/langchain-0.1.0-py3-none-any.whl.metadata\n",
      "  Using cached langchain-0.1.0-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting PyYAML>=5.3 (from langchain)\n",
      "  Obtaining dependency information for PyYAML>=5.3 from https://files.pythonhosted.org/packages/84/02/404de95ced348b73dd84f70e15a41843d817ff8c1744516bf78358f2ffd2/PyYAML-6.0.1-cp312-cp312-macosx_11_0_arm64.whl.metadata\n",
      "  Using cached PyYAML-6.0.1-cp312-cp312-macosx_11_0_arm64.whl.metadata (2.1 kB)\n",
      "Collecting SQLAlchemy<3,>=1.4 (from langchain)\n",
      "  Obtaining dependency information for SQLAlchemy<3,>=1.4 from https://files.pythonhosted.org/packages/63/d8/85a5af3429be2b4f875994398a49cf33501d880ac7ce65a85f23c1d41db6/SQLAlchemy-2.0.25-cp312-cp312-macosx_11_0_arm64.whl.metadata\n",
      "  Using cached SQLAlchemy-2.0.25-cp312-cp312-macosx_11_0_arm64.whl.metadata (9.6 kB)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in ./env/lib/python3.12/site-packages (from langchain) (3.9.1)\n",
      "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain)\n",
      "  Obtaining dependency information for dataclasses-json<0.7,>=0.5.7 from https://files.pythonhosted.org/packages/ae/53/8c006de775834cd4ea64a445402dc195caeebb77dc76b7defb9b3887cb0d/dataclasses_json-0.6.3-py3-none-any.whl.metadata\n",
      "  Using cached dataclasses_json-0.6.3-py3-none-any.whl.metadata (25 kB)\n",
      "Collecting jsonpatch<2.0,>=1.33 (from langchain)\n",
      "  Obtaining dependency information for jsonpatch<2.0,>=1.33 from https://files.pythonhosted.org/packages/73/07/02e16ed01e04a374e644b575638ec7987ae846d25ad97bcc9945a3ee4b0e/jsonpatch-1.33-py2.py3-none-any.whl.metadata\n",
      "  Using cached jsonpatch-1.33-py2.py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting langchain-community<0.1,>=0.0.9 (from langchain)\n",
      "  Obtaining dependency information for langchain-community<0.1,>=0.0.9 from https://files.pythonhosted.org/packages/80/18/33bf210e5410289b76da2862e6bcaf2217ee8c42ed0857e02741c4c02431/langchain_community-0.0.12-py3-none-any.whl.metadata\n",
      "  Downloading langchain_community-0.0.12-py3-none-any.whl.metadata (7.5 kB)\n",
      "Collecting langchain-core<0.2,>=0.1.7 (from langchain)\n",
      "  Obtaining dependency information for langchain-core<0.2,>=0.1.7 from https://files.pythonhosted.org/packages/5c/b9/95f91c284a9c316dbc1c6147076deede8452f147a067762828b125b50352/langchain_core-0.1.10-py3-none-any.whl.metadata\n",
      "  Using cached langchain_core-0.1.10-py3-none-any.whl.metadata (4.0 kB)\n",
      "Collecting langsmith<0.1.0,>=0.0.77 (from langchain)\n",
      "  Obtaining dependency information for langsmith<0.1.0,>=0.0.77 from https://files.pythonhosted.org/packages/f9/ed/1367bf56ad15c56f2cad40077ea6f9f040fc5939ed0fae28028fd67dbba2/langsmith-0.0.80-py3-none-any.whl.metadata\n",
      "  Using cached langsmith-0.0.80-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: numpy<2,>=1 in ./env/lib/python3.12/site-packages (from langchain) (1.26.3)\n",
      "Requirement already satisfied: pydantic<3,>=1 in ./env/lib/python3.12/site-packages (from langchain) (2.5.3)\n",
      "Requirement already satisfied: requests<3,>=2 in ./env/lib/python3.12/site-packages (from langchain) (2.31.0)\n",
      "Collecting tenacity<9.0.0,>=8.1.0 (from langchain)\n",
      "  Obtaining dependency information for tenacity<9.0.0,>=8.1.0 from https://files.pythonhosted.org/packages/f4/f1/990741d5bb2487d529d20a433210ffa136a367751e454214013b441c4575/tenacity-8.2.3-py3-none-any.whl.metadata\n",
      "  Using cached tenacity-8.2.3-py3-none-any.whl.metadata (1.0 kB)\n",
      "Requirement already satisfied: attrs>=17.3.0 in ./env/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.2.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in ./env/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in ./env/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.4)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in ./env/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in ./env/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
      "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain)\n",
      "  Obtaining dependency information for marshmallow<4.0.0,>=3.18.0 from https://files.pythonhosted.org/packages/57/e9/4368d49d3b462da16a3bac976487764a84dd85cef97232c7bd61f5bdedf3/marshmallow-3.20.2-py3-none-any.whl.metadata\n",
      "  Using cached marshmallow-3.20.2-py3-none-any.whl.metadata (7.5 kB)\n",
      "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain)\n",
      "  Obtaining dependency information for typing-inspect<1,>=0.4.0 from https://files.pythonhosted.org/packages/65/f3/107a22063bf27bdccf2024833d3445f4eea42b2e598abfbd46f6a63b6cb0/typing_inspect-0.9.0-py3-none-any.whl.metadata\n",
      "  Using cached typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting jsonpointer>=1.9 (from jsonpatch<2.0,>=1.33->langchain)\n",
      "  Obtaining dependency information for jsonpointer>=1.9 from https://files.pythonhosted.org/packages/12/f6/0232cc0c617e195f06f810534d00b74d2f348fe71b2118009ad8ad31f878/jsonpointer-2.4-py2.py3-none-any.whl.metadata\n",
      "  Using cached jsonpointer-2.4-py2.py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting anyio<5,>=3 (from langchain-core<0.2,>=0.1.7->langchain)\n",
      "  Obtaining dependency information for anyio<5,>=3 from https://files.pythonhosted.org/packages/bf/cd/d6d9bb1dadf73e7af02d18225cbd2c93f8552e13130484f1c8dcfece292b/anyio-4.2.0-py3-none-any.whl.metadata\n",
      "  Using cached anyio-4.2.0-py3-none-any.whl.metadata (4.6 kB)\n",
      "Requirement already satisfied: packaging<24.0,>=23.2 in ./env/lib/python3.12/site-packages (from langchain-core<0.2,>=0.1.7->langchain) (23.2)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in ./env/lib/python3.12/site-packages (from pydantic<3,>=1->langchain) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.14.6 in ./env/lib/python3.12/site-packages (from pydantic<3,>=1->langchain) (2.14.6)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in ./env/lib/python3.12/site-packages (from pydantic<3,>=1->langchain) (4.9.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./env/lib/python3.12/site-packages (from requests<3,>=2->langchain) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./env/lib/python3.12/site-packages (from requests<3,>=2->langchain) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./env/lib/python3.12/site-packages (from requests<3,>=2->langchain) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./env/lib/python3.12/site-packages (from requests<3,>=2->langchain) (2023.11.17)\n",
      "Collecting sniffio>=1.1 (from anyio<5,>=3->langchain-core<0.2,>=0.1.7->langchain)\n",
      "  Using cached sniffio-1.3.0-py3-none-any.whl (10 kB)\n",
      "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain)\n",
      "  Using cached mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
      "Using cached langchain-0.1.0-py3-none-any.whl (797 kB)\n",
      "Using cached dataclasses_json-0.6.3-py3-none-any.whl (28 kB)\n",
      "Using cached jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
      "Downloading langchain_community-0.0.12-py3-none-any.whl (1.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m0m\n",
      "\u001b[?25hUsing cached langchain_core-0.1.10-py3-none-any.whl (216 kB)\n",
      "Using cached langsmith-0.0.80-py3-none-any.whl (48 kB)\n",
      "Using cached PyYAML-6.0.1-cp312-cp312-macosx_11_0_arm64.whl (165 kB)\n",
      "Using cached SQLAlchemy-2.0.25-cp312-cp312-macosx_11_0_arm64.whl (2.1 MB)\n",
      "Using cached tenacity-8.2.3-py3-none-any.whl (24 kB)\n",
      "Using cached anyio-4.2.0-py3-none-any.whl (85 kB)\n",
      "Using cached jsonpointer-2.4-py2.py3-none-any.whl (7.8 kB)\n",
      "Using cached marshmallow-3.20.2-py3-none-any.whl (49 kB)\n",
      "Using cached typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
      "Installing collected packages: tenacity, SQLAlchemy, sniffio, PyYAML, mypy-extensions, marshmallow, jsonpointer, typing-inspect, jsonpatch, anyio, langsmith, dataclasses-json, langchain-core, langchain-community, langchain\n",
      "Successfully installed PyYAML-6.0.1 SQLAlchemy-2.0.25 anyio-4.2.0 dataclasses-json-0.6.3 jsonpatch-1.33 jsonpointer-2.4 langchain-0.1.0 langchain-community-0.0.12 langchain-core-0.1.10 langsmith-0.0.80 marshmallow-3.20.2 mypy-extensions-1.0.0 sniffio-1.3.0 tenacity-8.2.3 typing-inspect-0.9.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Collecting nltk\n",
      "  Using cached nltk-3.8.1-py3-none-any.whl (1.5 MB)\n",
      "Requirement already satisfied: click in ./env/lib/python3.12/site-packages (from nltk) (8.1.7)\n",
      "Collecting joblib (from nltk)\n",
      "  Obtaining dependency information for joblib from https://files.pythonhosted.org/packages/10/40/d551139c85db202f1f384ba8bcf96aca2f329440a844f924c8a0040b6d02/joblib-1.3.2-py3-none-any.whl.metadata\n",
      "  Using cached joblib-1.3.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting regex>=2021.8.3 (from nltk)\n",
      "  Obtaining dependency information for regex>=2021.8.3 from https://files.pythonhosted.org/packages/66/65/90e759a89534b850fa20e533e587748e967c44f58333b40f6d62718df1b1/regex-2023.12.25-cp312-cp312-macosx_11_0_arm64.whl.metadata\n",
      "  Using cached regex-2023.12.25-cp312-cp312-macosx_11_0_arm64.whl.metadata (40 kB)\n",
      "Requirement already satisfied: tqdm in ./env/lib/python3.12/site-packages (from nltk) (4.66.1)\n",
      "Using cached regex-2023.12.25-cp312-cp312-macosx_11_0_arm64.whl (292 kB)\n",
      "Using cached joblib-1.3.2-py3-none-any.whl (302 kB)\n",
      "Installing collected packages: regex, joblib, nltk\n",
      "Successfully installed joblib-1.3.2 nltk-3.8.1 regex-2023.12.25\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Collecting openai==0.28\n",
      "  Obtaining dependency information for openai==0.28 from https://files.pythonhosted.org/packages/ae/59/911d6e5f1d7514d79c527067643376cddcf4cb8d1728e599b3b03ab51c69/openai-0.28.0-py3-none-any.whl.metadata\n",
      "  Using cached openai-0.28.0-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: requests>=2.20 in ./env/lib/python3.12/site-packages (from openai==0.28) (2.31.0)\n",
      "Requirement already satisfied: tqdm in ./env/lib/python3.12/site-packages (from openai==0.28) (4.66.1)\n",
      "Requirement already satisfied: aiohttp in ./env/lib/python3.12/site-packages (from openai==0.28) (3.9.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./env/lib/python3.12/site-packages (from requests>=2.20->openai==0.28) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./env/lib/python3.12/site-packages (from requests>=2.20->openai==0.28) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./env/lib/python3.12/site-packages (from requests>=2.20->openai==0.28) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./env/lib/python3.12/site-packages (from requests>=2.20->openai==0.28) (2023.11.17)\n",
      "Requirement already satisfied: attrs>=17.3.0 in ./env/lib/python3.12/site-packages (from aiohttp->openai==0.28) (23.2.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in ./env/lib/python3.12/site-packages (from aiohttp->openai==0.28) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in ./env/lib/python3.12/site-packages (from aiohttp->openai==0.28) (1.9.4)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in ./env/lib/python3.12/site-packages (from aiohttp->openai==0.28) (1.4.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in ./env/lib/python3.12/site-packages (from aiohttp->openai==0.28) (1.3.1)\n",
      "Using cached openai-0.28.0-py3-none-any.whl (76 kB)\n",
      "Installing collected packages: openai\n",
      "Successfully installed openai-0.28.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Collecting pandas\n",
      "  Obtaining dependency information for pandas from https://files.pythonhosted.org/packages/0b/e0/8d97c7ecd73624f4cd5755578990b3cfbc6bbe350b8e017ede3580173a6f/pandas-2.1.4-cp312-cp312-macosx_11_0_arm64.whl.metadata\n",
      "  Using cached pandas-2.1.4-cp312-cp312-macosx_11_0_arm64.whl.metadata (18 kB)\n",
      "Requirement already satisfied: numpy<2,>=1.26.0 in ./env/lib/python3.12/site-packages (from pandas) (1.26.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./env/lib/python3.12/site-packages (from pandas) (2.8.2)\n",
      "Collecting pytz>=2020.1 (from pandas)\n",
      "  Obtaining dependency information for pytz>=2020.1 from https://files.pythonhosted.org/packages/32/4d/aaf7eff5deb402fd9a24a1449a8119f00d74ae9c2efa79f8ef9994261fc2/pytz-2023.3.post1-py2.py3-none-any.whl.metadata\n",
      "  Using cached pytz-2023.3.post1-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.1 (from pandas)\n",
      "  Obtaining dependency information for tzdata>=2022.1 from https://files.pythonhosted.org/packages/a3/fb/52b62131e21b24ee297e4e95ed41eba29647dad0e0051a92bb66b43c70ff/tzdata-2023.4-py2.py3-none-any.whl.metadata\n",
      "  Using cached tzdata-2023.4-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Requirement already satisfied: six>=1.5 in ./env/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Using cached pandas-2.1.4-cp312-cp312-macosx_11_0_arm64.whl (10.6 MB)\n",
      "Using cached pytz-2023.3.post1-py2.py3-none-any.whl (502 kB)\n",
      "Using cached tzdata-2023.4-py2.py3-none-any.whl (346 kB)\n",
      "Installing collected packages: pytz, tzdata, pandas\n",
      "Successfully installed pandas-2.1.4 pytz-2023.3.post1 tzdata-2023.4\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Collecting pdfminer\n",
      "  Using cached pdfminer-20191125-py3-none-any.whl\n",
      "Collecting pycryptodome (from pdfminer)\n",
      "  Obtaining dependency information for pycryptodome from https://files.pythonhosted.org/packages/ff/96/b0d494defb3346378086848a8ece5ddfd138a66c4a05e038fca873b2518c/pycryptodome-3.20.0-cp35-abi3-macosx_10_9_universal2.whl.metadata\n",
      "  Using cached pycryptodome-3.20.0-cp35-abi3-macosx_10_9_universal2.whl.metadata (3.4 kB)\n",
      "Using cached pycryptodome-3.20.0-cp35-abi3-macosx_10_9_universal2.whl (2.4 MB)\n",
      "Installing collected packages: pycryptodome, pdfminer\n",
      "Successfully installed pdfminer-20191125 pycryptodome-3.20.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Collecting pdfminer.six\n",
      "  Obtaining dependency information for pdfminer.six from https://files.pythonhosted.org/packages/eb/9c/e46fe7502b32d7db6af6e36a9105abb93301fa1ec475b5ddcba8b35ae23a/pdfminer.six-20231228-py3-none-any.whl.metadata\n",
      "  Using cached pdfminer.six-20231228-py3-none-any.whl.metadata (4.2 kB)\n",
      "Requirement already satisfied: charset-normalizer>=2.0.0 in ./env/lib/python3.12/site-packages (from pdfminer.six) (3.3.2)\n",
      "Collecting cryptography>=36.0.0 (from pdfminer.six)\n",
      "  Obtaining dependency information for cryptography>=36.0.0 from https://files.pythonhosted.org/packages/e4/73/5461318abd2fe426855a2f66775c063bbefd377729ece3c3ee048ddf19a5/cryptography-41.0.7-cp37-abi3-macosx_10_12_universal2.whl.metadata\n",
      "  Using cached cryptography-41.0.7-cp37-abi3-macosx_10_12_universal2.whl.metadata (5.2 kB)\n",
      "Collecting cffi>=1.12 (from cryptography>=36.0.0->pdfminer.six)\n",
      "  Obtaining dependency information for cffi>=1.12 from https://files.pythonhosted.org/packages/b4/f6/b28d2bfb5fca9e8f9afc9d05eae245bed9f6ba5c2897fefee7a9abeaf091/cffi-1.16.0-cp312-cp312-macosx_11_0_arm64.whl.metadata\n",
      "  Using cached cffi-1.16.0-cp312-cp312-macosx_11_0_arm64.whl.metadata (1.5 kB)\n",
      "Collecting pycparser (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six)\n",
      "  Using cached pycparser-2.21-py2.py3-none-any.whl (118 kB)\n",
      "Using cached pdfminer.six-20231228-py3-none-any.whl (5.6 MB)\n",
      "Using cached cryptography-41.0.7-cp37-abi3-macosx_10_12_universal2.whl (5.3 MB)\n",
      "Using cached cffi-1.16.0-cp312-cp312-macosx_11_0_arm64.whl (177 kB)\n",
      "Installing collected packages: pycparser, cffi, cryptography, pdfminer.six\n",
      "Successfully installed cffi-1.16.0 cryptography-41.0.7 pdfminer.six-20231228 pycparser-2.21\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Collecting plotly\n",
      "  Obtaining dependency information for plotly from https://files.pythonhosted.org/packages/a8/07/72953cf70e3bd3a24cbc3e743e6f8539abe6e3e6d83c3c0c83426eaffd39/plotly-5.18.0-py3-none-any.whl.metadata\n",
      "  Using cached plotly-5.18.0-py3-none-any.whl.metadata (7.0 kB)\n",
      "Requirement already satisfied: tenacity>=6.2.0 in ./env/lib/python3.12/site-packages (from plotly) (8.2.3)\n",
      "Requirement already satisfied: packaging in ./env/lib/python3.12/site-packages (from plotly) (23.2)\n",
      "Using cached plotly-5.18.0-py3-none-any.whl (15.6 MB)\n",
      "Installing collected packages: plotly\n",
      "Successfully installed plotly-5.18.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Collecting scikit-learn\n",
      "  Obtaining dependency information for scikit-learn from https://files.pythonhosted.org/packages/cf/fc/6c52ffeb587259b6b893b7cac268f1eb1b5426bcce1aa20e53523bfe6944/scikit_learn-1.3.2-cp312-cp312-macosx_12_0_arm64.whl.metadata\n",
      "  Using cached scikit_learn-1.3.2-cp312-cp312-macosx_12_0_arm64.whl.metadata (11 kB)\n",
      "Requirement already satisfied: numpy<2.0,>=1.17.3 in ./env/lib/python3.12/site-packages (from scikit-learn) (1.26.3)\n",
      "Collecting scipy>=1.5.0 (from scikit-learn)\n",
      "  Obtaining dependency information for scipy>=1.5.0 from https://files.pythonhosted.org/packages/5e/43/abf331745a7e5f4af51f13d40e2a72f516048db41ecbcf3ac6f86ada54a3/scipy-1.11.4-cp312-cp312-macosx_12_0_arm64.whl.metadata\n",
      "  Using cached scipy-1.11.4-cp312-cp312-macosx_12_0_arm64.whl.metadata (217 kB)\n",
      "Requirement already satisfied: joblib>=1.1.1 in ./env/lib/python3.12/site-packages (from scikit-learn) (1.3.2)\n",
      "Collecting threadpoolctl>=2.0.0 (from scikit-learn)\n",
      "  Obtaining dependency information for threadpoolctl>=2.0.0 from https://files.pythonhosted.org/packages/81/12/fd4dea011af9d69e1cad05c75f3f7202cdcbeac9b712eea58ca779a72865/threadpoolctl-3.2.0-py3-none-any.whl.metadata\n",
      "  Using cached threadpoolctl-3.2.0-py3-none-any.whl.metadata (10.0 kB)\n",
      "Using cached scikit_learn-1.3.2-cp312-cp312-macosx_12_0_arm64.whl (9.3 MB)\n",
      "Using cached scipy-1.11.4-cp312-cp312-macosx_12_0_arm64.whl (29.6 MB)\n",
      "Using cached threadpoolctl-3.2.0-py3-none-any.whl (15 kB)\n",
      "Installing collected packages: threadpoolctl, scipy, scikit-learn\n",
      "Successfully installed scikit-learn-1.3.2 scipy-1.11.4 threadpoolctl-3.2.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Collecting tiktoken\n",
      "  Obtaining dependency information for tiktoken from https://files.pythonhosted.org/packages/91/13/c998aa4f53343fb2e7ec6cbfeff23a57623e774e518c033c2a675a935afb/tiktoken-0.5.2-cp312-cp312-macosx_11_0_arm64.whl.metadata\n",
      "  Using cached tiktoken-0.5.2-cp312-cp312-macosx_11_0_arm64.whl.metadata (6.6 kB)\n",
      "Requirement already satisfied: regex>=2022.1.18 in ./env/lib/python3.12/site-packages (from tiktoken) (2023.12.25)\n",
      "Requirement already satisfied: requests>=2.26.0 in ./env/lib/python3.12/site-packages (from tiktoken) (2.31.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./env/lib/python3.12/site-packages (from requests>=2.26.0->tiktoken) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./env/lib/python3.12/site-packages (from requests>=2.26.0->tiktoken) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./env/lib/python3.12/site-packages (from requests>=2.26.0->tiktoken) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./env/lib/python3.12/site-packages (from requests>=2.26.0->tiktoken) (2023.11.17)\n",
      "Using cached tiktoken-0.5.2-cp312-cp312-macosx_11_0_arm64.whl (953 kB)\n",
      "Installing collected packages: tiktoken\n",
      "Successfully installed tiktoken-0.5.2\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "\u001b[31mERROR: Could not find a version that satisfies the requirement torch (from versions: none)\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: No matching distribution found for torch\u001b[0m\u001b[31m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Collecting transformers\n",
      "  Obtaining dependency information for transformers from https://files.pythonhosted.org/packages/20/0a/739426a81f7635b422fbe6cb8d1d99d1235579a6ac8024c13d743efa6847/transformers-4.36.2-py3-none-any.whl.metadata\n",
      "  Using cached transformers-4.36.2-py3-none-any.whl.metadata (126 kB)\n",
      "Collecting filelock (from transformers)\n",
      "  Obtaining dependency information for filelock from https://files.pythonhosted.org/packages/81/54/84d42a0bee35edba99dee7b59a8d4970eccdd44b99fe728ed912106fc781/filelock-3.13.1-py3-none-any.whl.metadata\n",
      "  Using cached filelock-3.13.1-py3-none-any.whl.metadata (2.8 kB)\n",
      "Collecting huggingface-hub<1.0,>=0.19.3 (from transformers)\n",
      "  Obtaining dependency information for huggingface-hub<1.0,>=0.19.3 from https://files.pythonhosted.org/packages/3d/0a/aed3253a9ce63d9c90829b1d36bc44ad966499ff4f5827309099c8c9184b/huggingface_hub-0.20.2-py3-none-any.whl.metadata\n",
      "  Using cached huggingface_hub-0.20.2-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in ./env/lib/python3.12/site-packages (from transformers) (1.26.3)\n",
      "Requirement already satisfied: packaging>=20.0 in ./env/lib/python3.12/site-packages (from transformers) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in ./env/lib/python3.12/site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in ./env/lib/python3.12/site-packages (from transformers) (2023.12.25)\n",
      "Requirement already satisfied: requests in ./env/lib/python3.12/site-packages (from transformers) (2.31.0)\n",
      "Collecting tokenizers<0.19,>=0.14 (from transformers)\n",
      "  Obtaining dependency information for tokenizers<0.19,>=0.14 from https://files.pythonhosted.org/packages/98/38/72b818a4f0d07c5d51706719b9223ee82dc327ac8c7931aeef2652f20ed3/tokenizers-0.15.0-cp312-cp312-macosx_11_0_arm64.whl.metadata\n",
      "  Using cached tokenizers-0.15.0-cp312-cp312-macosx_11_0_arm64.whl.metadata (6.7 kB)\n",
      "Collecting safetensors>=0.3.1 (from transformers)\n",
      "  Obtaining dependency information for safetensors>=0.3.1 from https://files.pythonhosted.org/packages/fa/be/ea93dce978fd9adadbd1fae8aa62682c88c59dfe5c2ae52e77c09c2ff461/safetensors-0.4.1-cp312-cp312-macosx_11_0_arm64.whl.metadata\n",
      "  Using cached safetensors-0.4.1-cp312-cp312-macosx_11_0_arm64.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: tqdm>=4.27 in ./env/lib/python3.12/site-packages (from transformers) (4.66.1)\n",
      "Collecting fsspec>=2023.5.0 (from huggingface-hub<1.0,>=0.19.3->transformers)\n",
      "  Obtaining dependency information for fsspec>=2023.5.0 from https://files.pythonhosted.org/packages/70/25/fab23259a52ece5670dcb8452e1af34b89e6135ecc17cd4b54b4b479eac6/fsspec-2023.12.2-py3-none-any.whl.metadata\n",
      "  Using cached fsspec-2023.12.2-py3-none-any.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in ./env/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.9.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./env/lib/python3.12/site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./env/lib/python3.12/site-packages (from requests->transformers) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./env/lib/python3.12/site-packages (from requests->transformers) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./env/lib/python3.12/site-packages (from requests->transformers) (2023.11.17)\n",
      "Using cached transformers-4.36.2-py3-none-any.whl (8.2 MB)\n",
      "Using cached huggingface_hub-0.20.2-py3-none-any.whl (330 kB)\n",
      "Using cached safetensors-0.4.1-cp312-cp312-macosx_11_0_arm64.whl (425 kB)\n",
      "Using cached tokenizers-0.15.0-cp312-cp312-macosx_11_0_arm64.whl (2.5 MB)\n",
      "Using cached filelock-3.13.1-py3-none-any.whl (11 kB)\n",
      "Using cached fsspec-2023.12.2-py3-none-any.whl (168 kB)\n",
      "Installing collected packages: safetensors, fsspec, filelock, huggingface-hub, tokenizers, transformers\n",
      "Successfully installed filelock-3.13.1 fsspec-2023.12.2 huggingface-hub-0.20.2 safetensors-0.4.1 tokenizers-0.15.0 transformers-4.36.2\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Requirement already satisfied: tqdm in ./env/lib/python3.12/site-packages (4.66.1)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# MASTER-CODEBLOCK\n",
    "##################################\n",
    "# Block 000 Initialize Dependencies\n",
    "#### REQUIREMENTS.TXT (reference)\n",
    "\n",
    "! pip install \"deeplake[enterprise]\"\n",
    "! pip install langchain\n",
    "! pip install nltk\n",
    "! pip install openai==0.28\n",
    "! pip install pandas\n",
    "! pip install pdfminer\n",
    "! pip install pdfminer.six\n",
    "! pip install plotly\n",
    "! pip install -U scikit-learn\n",
    "! pip install tiktoken\n",
    "! pip install torch\n",
    "! pip install transformers\n",
    "! pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/matthewsimon/Documents/GitHub/acdc.vendure_v2/env/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n"
     ]
    }
   ],
   "source": [
    "# MASTER-CODEBLOCK\n",
    "##################################\n",
    "# Block 001 Initialize Import Statements\n",
    "#### IMPORT STATEMENTS (reference)\n",
    "\n",
    "import os\n",
    "import json\n",
    "# import matplotlib\n",
    "# import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "import numpy as np\n",
    "import openai\n",
    "import pandas as pd\n",
    "import re\n",
    "# import torch\n",
    "from collections import Counter\n",
    "from deeplake.core.vectorstore import VectorStore\n",
    "from langchain.chains import RetrievalQA, ConversationalRetrievalChain\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.memory import ConversationTokenBufferMemory\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.retrievers.self_query.base import SelfQueryRetriever\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.vectorstores import DeepLake\n",
    "from nltk.corpus import stopwords\n",
    "import pandas as pd\n",
    "from pdfminer.high_level import extract_text, extract_pages\n",
    "from sklearn.cluster import KMeans\n",
    "from tqdm import tqdm\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer, GPT2TokenizerFast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/matthewsimon/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Vendure-Connect-API.pdf...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  2.42it/s]\n"
     ]
    }
   ],
   "source": [
    "# MASTER-CODEBLOCK\n",
    "##################################\n",
    "# Block 002 Initialize Pandas Database\n",
    "#### CREATE Pandas-DB INDEX\n",
    "\n",
    "# Download necessary NLTK data\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Stop words from NLTK\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Set the max cell size for text (32,767 characters = true limit)\n",
    "MAX_CELL_SIZE = 11250\n",
    "\n",
    "# Directory containing your PDFs\n",
    "pdf_directory = '/Users/matthewsimon/Documents/GitHub/acdc.vendure_v2/data/source_docs'\n",
    "\n",
    "# List to store data\n",
    "data = []\n",
    "\n",
    "# Wrap the loop with tqdm for a progress bar\n",
    "for pdf_file in tqdm(os.listdir(pdf_directory)):\n",
    "    if pdf_file.endswith('.pdf'):\n",
    "        file_path = os.path.join(pdf_directory, pdf_file)\n",
    "        try:\n",
    "            print(f\"Processing {pdf_file}...\")\n",
    "            text = extract_text(file_path)\n",
    "\n",
    "            if not text:\n",
    "                print(f\"Extracted text is empty for {pdf_file}\")\n",
    "                continue\n",
    "\n",
    "            text_words_set = set(text.lower().split())\n",
    "            filtered_words_set = text_words_set - stop_words\n",
    "            filtered_text = ' '.join(filtered_words_set)\n",
    "\n",
    "            # Basic heuristic: Assuming title is the first line and summary is the second line\n",
    "            lines = text.split('\\n')\n",
    "            title = lines[0] if len(lines) > 0 else ''\n",
    "            summary = lines[1] if len(lines) > 1 else ''\n",
    "\n",
    "            # Metadata extraction\n",
    "            file_size = os.path.getsize(file_path)\n",
    "            number_of_pages = len(list(extract_pages(file_path)))\n",
    "\n",
    "            # Filter Stopwords\n",
    "            text_words = text.split()\n",
    "            filtered_words = [word for word in text_words if word.lower() not in stop_words]\n",
    "            filtered_text = ' '.join(filtered_words)\n",
    "\n",
    "            # Text normalization\n",
    "            text = text.lower()\n",
    "\n",
    "            # Chunking the content\n",
    "            chunks = [filtered_text[i:i+MAX_CELL_SIZE] for i in range(0, len(filtered_text), MAX_CELL_SIZE)]\n",
    "            for chunk in chunks:\n",
    "                data.append({\n",
    "                    'filename': pdf_file,\n",
    "                    'title_or_heading': title,\n",
    "                    'content_summary': summary,\n",
    "                    'content_chunk': chunk,\n",
    "                    'file_size': file_size,\n",
    "                    'number_of_pages': number_of_pages\n",
    "                })\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {pdf_file}: {e}\")\n",
    "            continue\n",
    "\n",
    "# Convert the list to a DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Save the DataFrame to a CSV for further analysis with escapechar\n",
    "df.to_csv('/Users/matthewsimon/Documents/GitHub/acdc.vendure_v2/data/source_csv/source_docs.csv', index=False, escapechar='\\\\')\n",
    "\n",
    "## print(\"Listing directory contents:\")\n",
    "## print(os.listdir(pdf_directory))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/matthewsimon/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['filename', 'title_or_heading', 'content_summary', 'content_chunk',\n",
      "       'file_size', 'cleaned_content_chunk', 'n_tokens', 'tokens',\n",
      "       'decoded_tokens', 'ada_similarity'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# MASTER-CODEBLOCK\n",
    "##################################\n",
    "# Block 003 Initialize Embeddings & Update.df \n",
    "#### UPDATE PANDAS.DB INDEX WITH NEW EMBEDDINGS COLUMN \n",
    "\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = text.split()\n",
    "    cleaned_text = \" \".join([word for word in words if word.lower() not in stop_words])\n",
    "    return cleaned_text\n",
    "\n",
    "# Your existing setup (msimon@acdc.digital)\n",
    "openai.api_key = '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'\n",
    "input_datapath = '/Users/matthewsimon/Documents/GitHub/acdc.vendure_v2/data/source_csv/source_docs.csv'\n",
    "df_check = pd.read_csv(input_datapath)\n",
    "df = pd.read_csv(input_datapath)\n",
    "\n",
    "# Check if DataFrame is empty\n",
    "if df.empty:\n",
    "    print(\"The DataFrame is empty. Please check your data source.\")\n",
    "else:\n",
    "    # Your existing setup\n",
    "    df = df[['filename', 'title_or_heading', 'content_summary', 'content_chunk', 'file_size']]\n",
    "\n",
    "    # Remove stop words from 'content_chunk'\n",
    "    df['cleaned_content_chunk'] = df['content_chunk'].apply(remove_stopwords)\n",
    "\n",
    "    # Initialize the tokenizer\n",
    "    tokenizer = GPT2TokenizerFast.from_pretrained(\"gpt2\")\n",
    "\n",
    "    # Function to return token IDs and decoded tokens\n",
    "    def get_tokens_and_decoded(text):\n",
    "        token_ids = tokenizer.encode(text, truncation=True, max_length=4095)\n",
    "        decoded_tokens = [tokenizer.decode([token_id]) for token_id in token_ids]\n",
    "        return token_ids, decoded_tokens\n",
    "\n",
    "    # Add new columns for token counts, token IDs, and decoded tokens\n",
    "    df['n_tokens'], df['tokens'] = zip(*df['cleaned_content_chunk'].apply(lambda x: (len(get_tokens_and_decoded(x)[0]), get_tokens_and_decoded(x)[0])))\n",
    "    df['decoded_tokens'] = df['cleaned_content_chunk'].apply(lambda x: get_tokens_and_decoded(x)[1])\n",
    "\n",
    "    # Filter rows based on token count\n",
    "    df = df[df.n_tokens < 5000]\n",
    "\n",
    "# Define the function to get embeddings using OpenAI's API\n",
    "def get_embedding(text, engine):\n",
    "    response = openai.Embedding.create(input=[text], engine=engine)\n",
    "    return response['data'][0]['embedding']\n",
    "\n",
    "# Generate embeddings\n",
    "df['ada_similarity'] = df['cleaned_content_chunk'].apply(lambda x: get_embedding(x, engine='text-embedding-ada-002'))\n",
    "\n",
    "# Save the DataFrame to a new CSV file\n",
    "df.to_csv('/Users/matthewsimon/Documents/GitHub/acdc.vendure_v2/data/source_ada/source_ada.csv', index=False)\n",
    "# Read the new CSV file to verify\n",
    "df_new = pd.read_csv('/Users/matthewsimon/Documents/GitHub/acdc.vendure_v2/data/source_ada/source_ada.csv')\n",
    "\n",
    "print(df_new.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your Deep Lake dataset has been successfully created!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploading data to deeplake dataset.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:01<00:00,  1.12s/it]\n",
      "-"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset(path='hub://solomon/vendure-io', tensors=['text', 'metadata', 'embedding', 'id'])\n",
      "\n",
      "  tensor      htype      shape     dtype  compression\n",
      "  -------    -------    -------   -------  ------- \n",
      "   text       text      (1, 1)      str     None   \n",
      " metadata     json      (1, 1)      str     None   \n",
      " embedding  embedding  (1, 1536)  float32   None   \n",
      "    id        text      (1, 1)      str     None   \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " \r"
     ]
    }
   ],
   "source": [
    "# MASTER-CODEBLOCK\n",
    "##################################\n",
    "# Block 004 Initialize Pandas & Activeloop DeepLake Database\n",
    "#### LOAD PANDAS.DB TO DEEPLAKE\n",
    "\n",
    "os.environ['ACTIVELOOP_TOKEN'] = 'eyJhbGciOiJIUzUxMiIsImlhdCI6MTcwNTA2NTU3MCwiZXhwIjoxNzM2Njg3OTUzfQ.eyJpZCI6ImFjZGNkaWdpdGFsIn0.C_L4DdFz7lKodj5MjMDmUZJLWrOmZn0AISyRBDQ5Qsi81QqOoOBJtaV4xvfTSqzusTkwz-SJ0IHCBgjwBcuQeQ'\n",
    "# Load DataFrame from CSV\n",
    "df = pd.read_csv('/Users/matthewsimon/Documents/GitHub/acdc.vendure_v2/data/source_ada/source_ada.csv')\n",
    "\n",
    "# Prepare data\n",
    "chunked_text = df['content_chunk'].tolist()\n",
    "source_texts = df['filename'].tolist()\n",
    "precomputed_embeddings = df['ada_similarity'].apply(eval).tolist()  # Assuming embeddings are stored as strings\n",
    "\n",
    "# Initialize Vector Store with the Hub URL\n",
    "vector_store_path = \"hub://solomon/vendure-io\"\n",
    "vector_store = VectorStore(\n",
    "    path=vector_store_path,\n",
    ")\n",
    "\n",
    "# Add data to Vector Store\n",
    "vector_store.add(\n",
    "    text=chunked_text,\n",
    "    embedding=precomputed_embeddings,\n",
    "    metadata=[{\"source\": source_text} for source_text in source_texts]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deep Lake Dataset in hub://solomon/vendure-io already exists, loading from the storage\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/matthewsimon/Documents/GitHub/acdc.vendure_v2/env/lib/python3.12/site-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The class `langchain_community.chat_models.openai.ChatOpenAI` was deprecated in langchain-community 0.0.10 and will be removed in 0.2.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import ChatOpenAI`.\n",
      "  warn_deprecated(\n",
      "/Users/matthewsimon/Documents/GitHub/acdc.vendure_v2/env/lib/python3.12/site-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The function `__call__` was deprecated in LangChain 0.1.0 and will be removed in 0.2.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "QA Response:\n",
      "Absolutely! To connect your Qwick storefront to the Vendure backend using the Vendure Shop API, you'll need to follow these general steps:\n",
      "\n",
      "1. **Set Up GraphQL Client:**\n",
      "   You will need a GraphQL client to interact with the Vendure Shop API. You can use TanStack Query with graphql-request as a client. To set it up, you can refer to the `src/client.ts` example provided in the context to configure the GraphQLClient with request and response middleware for handling authentication tokens.\n",
      "\n",
      "2. **Authentication:**\n",
      "   Vendure supports two methods for session management: cookie-based and bearer token. If you opt for bearer token, ensure that you're storing the token in localStorage and adding it to the headers of outgoing GraphQL requests.\n",
      "\n",
      "3. **Configure Vendure Client Settings:**\n",
      "   - Set up the `API_URL` to point to your Vendure server's Shop API endpoint.\n",
      "   - Configure the client to use credentials for cookie-based sessions if you're using cookies.\n",
      "   - Add request and response middleware to handle the bearer token if you're using token-based authentication.\n",
      "\n",
      "4. **Shop API Connection:**\n",
      "   Open the GraphQL Playground in your browser at `http://localhost:3000/shop-api` if your Vendure server is running locally. This allows you to explore the Shop API and test queries and mutations.\n",
      "\n",
      "5. **Session Management:**\n",
      "   Make sure to handle the session duration according to your requirements and configure the `authOptions` in your Vendure server's `src/vendure-config.ts` file appropriately.\n",
      "\n",
      "6. **Specify Channel and Language:**\n",
      "   If you're working with multiple channels, specify the active channel by setting the `vendure-token` header in your requests. To set the language for translations, include the `languageCode` in the query string or set it in the client configuration.\n",
      "\n",
      "7. **Code Generation:**\n",
      "   For a TypeScript-based storefront, set up code generation to ensure that the responses to your queries and mutations are correctly typed. This will help you avoid errors related to data types and structures.\n",
      "\n",
      "8. **Integrate Client with Storefront:**\n",
      "   In your storefront's main application file (`src/App.tsx` in the example), use the `useQuery` hook from `@tanstack/react-query` to fetch data from the Vendure Shop API. Use GraphQL queries, like the `GET_PRODUCTS` example, to retrieve the data you need for your storefront.\n",
      "\n",
      "9. **Launch the Storefront:**\n",
      "   Ensure that your storefront is correctly initialized and that the `QueryClientProvider` from `@tanstack/react-query` is wrapping your App component in `src/index.tsx`. This setup allows you to use the `useQuery` hook throughout your application.\n",
      "\n",
      "10. **Testing:**\n",
      "    Finally, test the connection between your storefront and the Vendure backend. Make sure that you can fetch products and that user sessions are being managed correctly, whether through cookies or bearer tokens.\n",
      "\n",
      "Remember to replace `http://localhost:3000/shop-api` and other placeholders with the actual URL and values for your production environment. Also, make sure to handle CORS and other security settings as needed for your deployment.\n",
      "\n",
      "If you encounter any issues during the process, refer back to the Vendure documentation, and check the console for any errors that might indicate what's going wrong.\n",
      "\n",
      "Number of tokens in the search query: 77\n",
      "Number of tokens in the prompt: 172\n",
      "Number of tokens in the generated response: 769\n",
      "\n",
      "Unique Sources:\n",
      "['Vendure-Connect-API.pdf']\n"
     ]
    }
   ],
   "source": [
    "# MASTER-CODEBLOCK\n",
    "##################################\n",
    "# Block 005 Initialize Conversational Chat\n",
    "#### EMBEDDING RETRIEVAL FOR DOCS-QA\n",
    "\n",
    "# Initialize OpenAI\n",
    "import os\n",
    "os.environ['OPENAI_API_KEY'] = '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'\n",
    "os.environ['ACTIVELOOP_TOKEN'] = 'eyJhbGciOiJIUzUxMiIsImlhdCI6MTcwNTA2NTU3MCwiZXhwIjoxNzM2Njg3OTUzfQ.eyJpZCI6ImFjZGNkaWdpdGFsIn0.C_L4DdFz7lKodj5MjMDmUZJLWrOmZn0AISyRBDQ5Qsi81QqOoOBJtaV4xvfTSqzusTkwz-SJ0IHCBgjwBcuQeQ'\n",
    "\n",
    "\n",
    "# Your embedding function\n",
    "def embedding_function(texts, model=\"text-embedding-ada-002\"):\n",
    "    if isinstance(texts, str):\n",
    "        texts = [texts]\n",
    "    texts = [t.replace(\"\\n\", \" \") for t in texts]\n",
    "    return [data['embedding'] for data in openai.Embedding.create(input=texts, model=model)['data']]\n",
    "\n",
    "# Wrap your function in a class with an embed_query method\n",
    "class MyEmbeddingFunction:\n",
    "    def __init__(self, func):\n",
    "        self.func = func\n",
    "\n",
    "    def embed_query(self, query):\n",
    "        return self.func(query)\n",
    "\n",
    "# Initialize DeepLake database with the embedding_function\n",
    "embedding_function_obj = MyEmbeddingFunction(embedding_function)\n",
    "db = DeepLake(dataset_path=\"hub://solomon/vendure-io\", embedding=embedding_function_obj, read_only=False)\n",
    "\n",
    "# Initialize Retriever with parameters\n",
    "retriever = db.as_retriever()\n",
    "retriever.search_kwargs.update({\n",
    "    'distance_metric': 'cos',\n",
    "    'k': 4\n",
    "})\n",
    "\n",
    "# Define the PromptTemplate\n",
    "template = \"\"\"\n",
    "You are Solomon. A new type of personal assistant. Your goal is to use your vast knowledge, and advanced artificial intellegence capabilities to help answer Users questions by retrieving and citing knowledge that may not have otherwise been identifiable with human capabilities alone. You must answer questions in a human-like manner. \n",
    "\n",
    "You can assume the User asking questions is an expert in the subject field of their question. Ensure to breakdown complex tasks into a sequence of manageable steps using your critical analysis. \n",
    "\n",
    "Use the following context to assist in answering any questions that come up. If you don't know the answer, or if the answer is not provided in the context in some way, just say that there's no relevant information within the context to answer the User's question. \n",
    "{context}\n",
    "Question: {question}\n",
    "Helpful Answer:\n",
    "\"\"\"\n",
    "\n",
    "# Create a PromptTemplate object\n",
    "QA_CHAIN_PROMPT = PromptTemplate.from_template(template)\n",
    "\n",
    "# Initialize LLM for QA\n",
    "model = ChatOpenAI(model='gpt-4-1106-preview')\n",
    "\n",
    "# Initialize Langchain Memory with Token Buffer\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "os.environ[\"LANGCHAIN_ENDPOINT\"] = \"https://api.langchain.plus\"\n",
    "os.environ[\"LANGCHAIN_API_KEY\"] = \"ls__fbfe7701decf42138ac5d036eb60afc5\"\n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = \"solomon.v2.2\"\n",
    "\n",
    "memory = ConversationTokenBufferMemory(  # <-- Changed to ConversationTokenBufferMemory\n",
    "    llm=model,\n",
    "    max_token_limit=450,\n",
    "    memory_key=\"chat_history\",\n",
    "    return_messages=True\n",
    ")\n",
    "\n",
    "# Initialize Conversational Retrieval Chain with Memory\n",
    "qa = ConversationalRetrievalChain.from_llm(\n",
    "    llm=model,\n",
    "    retriever=retriever,\n",
    "    memory=memory\n",
    ")\n",
    "\n",
    "# Define your search query\n",
    "search_query = 'Hi! Weve sucessfully initiated our Vendure.io backend/ admin pages, and weve now initated our Qwick storefront. We need to sucessfully connect the API to our frontend (storefront) and our backend (admin pages). Ive attahed the API-Connection docs for your reference. Are you able to guide me through this process?'\n",
    "\n",
    "# Count the number of tokens in the search query and prompt\n",
    "tokenizer = GPT2TokenizerFast.from_pretrained(\"gpt2\")\n",
    "search_query_tokens = tokenizer.encode(search_query, truncation=True)\n",
    "prompt_tokens = tokenizer.encode(template, truncation=True)\n",
    "num_search_query_tokens = len(search_query_tokens)\n",
    "num_prompt_tokens = len(prompt_tokens)\n",
    "\n",
    "# Run the QA model with top-k documents\n",
    "result = qa({\"question\": search_query})\n",
    "response = result['answer']\n",
    "print(\"\\nQA Response:\")\n",
    "print(response)\n",
    "\n",
    "# Count the number of tokens in the generated response\n",
    "response_tokens = tokenizer.encode(response, truncation=True)\n",
    "num_response_tokens = len(response_tokens)\n",
    "\n",
    "# Print token counts\n",
    "print(f\"\\nNumber of tokens in the search query: {num_search_query_tokens}\")\n",
    "print(f\"Number of tokens in the prompt: {num_prompt_tokens}\")\n",
    "print(f\"Number of tokens in the generated response: {num_response_tokens}\")\n",
    "\n",
    "# Extract and print unique sources (Top 3)\n",
    "print(\"\\nUnique Sources:\")\n",
    "docs = retriever.get_relevant_documents(search_query)\n",
    "unique_sources = set(doc.metadata.get('source', 'N/A') for doc in docs)\n",
    "unique_sources_top3 = list(unique_sources)[:3]\n",
    "print(unique_sources_top3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code for asking questions\n",
    "question = \"If I'm using the TanStack Query library, do I need to install anything additional for GraphQL? Is GraphQL it's own library that I need to download in order for Vendure to work, I don't understand why I need it or how to set it up in order to use Tanstack with it- can you please explain this process to me?\"\n",
    "result = qa({\"question\": question})\n",
    "print(result['answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code for asking questions\n",
    "question = \"Ok, so I need to install Tanstack and GraphQL - I'll use the graphql-request client. Will these installations create new directories within my project?\"\n",
    "result = qa({\"question\": question})\n",
    "print(result['answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code for asking questions\n",
    "question = \"So, in the Docs it shows examples for scripts to be modified in the Tanstack example section. Where are these scripts/ files which need to be modified?\"\n",
    "result = qa({\"question\": question})\n",
    "print(result['answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
